import streamlit as st
import os
import tempfile
import warnings
from huggingface_hub import hf_hub_download

# LangChain components
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # <-- ржПржЗ рж▓рж╛ржЗржиржЯрж┐ ржкрж░рж┐ржмрж░рзНрждржи ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Suppress warnings
warnings.filterwarnings('ignore')

# --- ржЕрзНржпрж╛ржкрзЗрж░ ржЯрж╛ржЗржЯрзЗрж▓ ржПржмржВ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ---
st.set_page_config(page_title="Local RAG Chatbot", page_icon="ЁЯдЦ")
st.title("ЁЯУДЁЯдЦ ржЕрзНржпрж╛ржбржнрж╛ржирзНрж╕ржб RAG ржЪрзНржпрж╛ржЯржмржЯ")
st.write("ржЖржкржирж╛рж░ PDF ржЖржкрж▓рзЛржб ржХрж░рзБржи (ржпрж╛рждрзЗ ржЯрзЗржХрзНрж╕ржЯ, ржЯрзЗржмрж┐рж▓ ржПржмржВ ржЫржмрж┐ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗ) ржПржмржВ ржкрзНрж░рж╢рзНржи ржХрж░рзБржиред")

# --- ржХрзНржпрж╛рж╢ржб ржлрж╛ржВрж╢ржи (ржпрж╛рждрзЗ рж░рж┐рж╕рзЛрж░рзНрж╕ ржмрж╛рж░ржмрж╛рж░ рж▓рзЛржб ржирж╛ рж╣рзЯ) ---

@st.cache_resource
def load_llm():
    """
    рж▓рзЛржХрж╛рж▓ GGUF LLM (Mistral-7B) рж▓рзЛржб ржХрж░рзЗред
    """
    try:
        st.write("Downloading local LLM (Mistral-7B-Instruct GGUF)... This may take a few minutes on first run.")
        # Mistral-7B (ржЧрзЗржЯрзЗржб ржирзЯ)
        model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
        model_file = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
        
        model_path = hf_hub_download(
            repo_id=model_name_or_path,
            filename=model_file
        )
        
        st.write("Loading LLM into memory...")
        # ржирзЛржЯ: рж▓рзЛржХрж╛рж▓ ржорзЗрж╢рж┐ржирзЗ GPU ржирж╛ ржерж╛ржХрж▓рзЗ 'gpu_layers' 0 рж░рж╛ржЦрзБржиред
        # ржЖржкржирж╛рж░ NVIDIA GPU ржерж╛ржХрж▓рзЗ ржПржЯрж┐ ржмрж╛рзЬрж╛рждрзЗ ржкрж╛рж░рзЗржи (ржпрзЗржоржи: 20-50)ред
        llm = CTransformers(
            model=model_path,
            model_type='mistral',
            config={
                'context_length': 4096,
                'gpu_layers': 0, # рж▓рзЛржХрж╛рж▓ CPU-ржПрж░ ржЬржирзНржп 0; GPU ржерж╛ржХрж▓рзЗ ржмрж╛рзЬрж╛ржи
                'temperature': 0.1
            }
        )
        st.write("Local LLM loaded successfully.")
        return llm
    except Exception as e:
        st.error(f"Error loading LLM: {e}")
        return None

@st.cache_resource
def create_rag_pipeline(uploaded_file, _embeddings):
    """
    ржЖржкрж▓рзЛржб ржХрж░рж╛ PDF ржерзЗржХрзЗ RAG ржкрж╛ржЗржкрж▓рж╛ржЗржи (ржнрзЗржХрзНржЯрж░ рж╕рзНржЯрзЛрж░ ржПржмржВ рж░рж┐ржЯрзНрж░рж┐ржнрж╛рж░) рждрзИрж░рж┐ ржХрж░рзЗред
    """
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        st.write(f"Processing PDF '{uploaded_file.name}' with OCR and table extraction...")
        # UnstructuredPDFLoader ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ рж╕ржм ржзрж░ржирзЗрж░ ржХржирзНржЯрзЗржирзНржЯ ржПржХрзНрж╕ржЯрзНрж░рж╛ржХрзНржЯ ржХрж░рж╛
        loader = UnstructuredPDFLoader(
            tmp_file_path, 
            strategy="hi_res", 
            mode="elements"
        )
        docs = loader.load()
        os.remove(tmp_file_path) # ржЯрзЗржорзНржкрзЛрж░рж╛рж░рж┐ ржлрж╛ржЗрж▓ ржбрж┐рж▓рж┐ржЯ ржХрж░рзБржи

        if not docs:
            st.error("Could not extract any content from the PDF.")
            return None

        st.write(f"Extracted {len(docs)} elements. Now creating vector store...")
        
        # Text Splitter
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        splitted_docs = text_splitter.split_documents(docs)
        
        # FAISS Vector Store (рж▓рзЛржХрж╛рж▓) - ржПржЦрж╛ржирзЗ faiss-cpu ржмрзНржпржмрж╣рзГржд рж╣ржЪрзНржЫрзЗ
        vector_store = FAISS.from_documents(splitted_docs, _embeddings)
        st.write("Vector store created successfully.")
        
        return vector_store
    except Exception as e:
        st.error(f"Error processing PDF: {e}")
        return None

@st.cache_resource
def load_embedding_model():
    """
    ржПржоржмрзЗржбрж┐ржВ ржоржбрзЗрж▓ рж▓рзЛржб ржХрж░рзЗред
    """
    st.write("Loading embedding model (all-MiniLM-L6-v2)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embeddings

# --- ржХрж╛рж╕рзНржЯржо ржкрзНрж░ржорзНржкржЯ ржЯрзЗржоржкрзНрж▓рзЗржЯ (Mistral-ржПрж░ ржЬржирзНржп) ---
prompt_template = """<s>[INST] You are a helpful assistant. Answer the user's question based only on the following context:

{context}

Question: {question} [/INST]
"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

# --- Streamlit ржЕрзНржпрж╛ржкрзЗрж░ ржкрзНрж░ржзрж╛ржи ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ ---

# рзз. рж░рж┐рж╕рзЛрж░рзНрж╕ рж▓рзЛржб ржХрж░рзБржи
embeddings = load_embedding_model()
llm = load_llm()

# рзи. ржЪрзНржпрж╛ржЯ рж╕рзЗрж╢ржи рж╕рзНржЯрзЗржЯ ржЗржирж┐рж╢рж┐рзЯрж╛рж▓рж╛ржЗржЬ ржХрж░рзБржи
if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# рзй. ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржбрж╛рж░
with st.sidebar:
    st.header("рзз. PDF ржЖржкрж▓рзЛржб ржХрж░рзБржи")
    uploaded_file = st.file_uploader("ржЖржкржирж╛рж░ PDF ржлрж╛ржЗрж▓ржЯрж┐ ржПржЦрж╛ржирзЗ ржЖржкрж▓рзЛржб ржХрж░рзБржи", type="pdf")
    
    if uploaded_file:
        if st.button("ржкрзНрж░рж╕рзЗрж╕ рж╢рзБрж░рзБ ржХрж░рзБржи"):
            with st.spinner("PDF ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ... (OCR ржПржмржВ ржЯрзЗржмрж┐рж▓ рж╕рж╣) ржПржЯрж╛рждрзЗ рж╕ржорзЯ рж▓рж╛ржЧрждрзЗ ржкрж╛рж░рзЗред"):
                # RAG ржкрж╛ржЗржкрж▓рж╛ржЗржи рждрзИрж░рж┐ ржХрж░рзБржи
                vector_store = create_rag_pipeline(uploaded_file, embeddings)
                
                if vector_store and llm:
                    # RAG ржЪрзЗржЗржи рждрзИрж░рж┐ ржХрж░рзБржи ржПржмржВ рж╕рзЗрж╢ржирзЗ рж╕рзЗржн ржХрж░рзБржи
                    st.session_state.qa_chain = RetrievalQA.from_chain_type(
                        llm=llm,
                        chain_type="stuff",
                        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
                        chain_type_kwargs={"prompt": PROMPT},
                        return_source_documents=True
                    )
                    st.session_state.messages = [] # ржирждрзБржи ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб рж╣рж▓рзЗ ржЪрзНржпрж╛ржЯ рж░рж┐рж╕рзЗржЯ ржХрж░рзБржи
                    st.success(f"'{uploaded_file.name}' рж╕ржлрж▓ржнрж╛ржмрзЗ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред ржПржЦржи ржЖржкржирж┐ ржкрзНрж░рж╢рзНржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржиред")
                else:
                    st.error("RAG ржкрж╛ржЗржкрж▓рж╛ржЗржи рждрзИрж░рж┐ ржХрж░рж╛ рж╕ржорзНржнржм рж╣рзЯржирж┐ред")

# рзк. ржЪрзНржпрж╛ржЯ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕
st.header("рзи. ржЪрзНржпрж╛ржЯ ржХрж░рзБржи")

if not st.session_state.qa_chain:
    st.warning("ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ рж╕рж╛ржЗржбржмрж╛рж░рзЗ ржПржХржЯрж┐ PDF ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб ржХрж░рзБржи ржПржмржВ 'ржкрзНрж░рж╕рзЗрж╕ рж╢рзБрж░рзБ ржХрж░рзБржи' ржмрж╛ржЯржирзЗ ржХрзНрж▓рж┐ржХ ржХрж░рзБржиред")

# ржЖржЧрзЗрж░ ржЪрзНржпрж╛ржЯ ржорзЗрж╕рзЗржЬржЧрзБрж▓рзЛ ржжрзЗржЦрж╛ржи
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ржЗржЙржЬрж╛рж░рзЗрж░ ржХрж╛ржЫ ржерзЗржХрзЗ ржирждрзБржи ржЗржиржкрзБржЯ ржирж┐ржи
if prompt := st.chat_input("ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи...", disabled=(not st.session_state.qa_chain)):
    # ржЗржЙржЬрж╛рж░ ржорзЗрж╕рзЗржЬ ржжрзЗржЦрж╛ржи
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI-ржПрж░ ржЙрждрзНрждрж░ ржЬрзЗржирж╛рж░рзЗржЯ ржХрж░рзБржи
    with st.chat_message("assistant"):
        with st.spinner("ржЙрждрзНрждрж░ ржЦрзЛржБржЬрж╛ рж╣ржЪрзНржЫрзЗ..."):
            try:
                result = st.session_state.qa_chain.invoke(prompt)
                response = result['result']
                
                # рж╕рзЛрж░рзНрж╕ ржбржХрзБржорзЗржирзНржЯ ржжрзЗржЦрж╛ржирзЛ (ржРржЪрзНржЫрж┐ржХ)
                if result.get('source_documents'):
                    with st.expander("рж╕рзЛрж░рзНрж╕ ржжрзЗржЦрзБржи"):
                        for i, doc in enumerate(result['source_documents']):
                            page = doc.metadata.get('page_number', 'N/A')
                            st.write(f"**рж╕рзЛрж░рзНрж╕ {i+1} (Page: {page})**")
                            st.caption(f"{doc.page_content[:200]}...")
                
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                st.error(f"An error occurred while generating response: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})
