import streamlit as st
import os
import tempfile
import warnings
from huggingface_hub import hf_hub_download

# LangChain components
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Suppress warnings
warnings.filterwarnings('ignore')

# --- ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡ßá‡¶∞ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
st.set_page_config(page_title="Local RAG Chatbot", page_icon="ü§ñ")
st.title("üìÑü§ñ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° RAG ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü")
st.write("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶Ø‡¶æ‡¶§‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶õ‡¶¨‡¶ø ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá) ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

# --- ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (‡¶Ø‡¶æ‡¶§‡ßá ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶≤‡ßã‡¶° ‡¶®‡¶æ ‡¶π‡ßü) ---

@st.cache_resource
def load_llm():
    """
    ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ GGUF LLM (Phi-3-mini) ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡•§
    """
    try:
        st.write("Downloading local LLM (Phi-3-mini GGUF)... This may take a few minutes on first run.")
        model_name_or_path = "microsoft/Phi-3-mini-4k-instruct-gguf"
        model_file = "Phi-3-mini-4k-instruct-Q4_K_M.gguf"
        
        model_path = hf_hub_download(
            repo_id=model_name_or_path,
            filename=model_file
        )
        
        st.write("Loading LLM into memory...")
        # ‡¶®‡ßã‡¶ü: ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶Æ‡ßá‡¶∂‡¶ø‡¶®‡ßá GPU ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá 'gpu_layers' 0 ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®‡•§
        # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ NVIDIA GPU ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶è‡¶ü‡¶ø ‡¶¨‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® (‡¶Ø‡ßá‡¶Æ‡¶®: 20-50)‡•§
        llm = CTransformers(
            model=model_path,
            model_type='llama',
            config={
                'context_length': 4096,
                'gpu_layers': 0, # ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ CPU-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 0; GPU ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶æ‡ßú‡¶æ‡¶®
                'temperature': 0.1
            }
        )
        st.write("Local LLM loaded successfully.")
        return llm
    except Exception as e:
        st.error(f"Error loading LLM: {e}")
        return None

@st.cache_resource
def create_rag_pipeline(uploaded_file, _embeddings):
    """
    ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ PDF ‡¶•‡ßá‡¶ï‡ßá RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® (‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶ü‡ßã‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠‡¶æ‡¶∞) ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§
    """
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        st.write(f"Processing PDF '{uploaded_file.name}' with OCR and table extraction...")
        # UnstructuredPDFLoader ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
        loader = UnstructuredPDFLoader(
            tmp_file_path, 
            strategy="hi_res", 
            mode="elements"
        )
        docs = loader.load()
        os.remove(tmp_file_path) # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡¶æ‡¶∞‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®

        if not docs:
            st.error("Could not extract any content from the PDF.")
            return None

        st.write(f"Extracted {len(docs)} elements. Now creating vector store...")
        
        # Text Splitter
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        splitted_docs = text_splitter.split_documents(docs)
        
        # FAISS Vector Store (‡¶≤‡ßã‡¶ï‡¶æ‡¶≤) - ‡¶è‡¶ñ‡¶æ‡¶®‡ßá faiss-cpu ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶ö‡ßç‡¶õ‡ßá
        vector_store = FAISS.from_documents(splitted_docs, _embeddings)
        st.write("Vector store created successfully.")
        
        return vector_store
    except Exception as e:
        st.error(f"Error processing PDF: {e}")
        return None

@st.cache_resource
def load_embedding_model():
    """
    ‡¶è‡¶Æ‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡•§
    """
    st.write("Loading embedding model (all-MiniLM-L6-v2)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embeddings

# --- ‡¶ï‡¶æ‡¶∏‡ßç‡¶ü‡¶Æ ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶ü‡ßá‡¶Æ‡¶™‡ßç‡¶≤‡ßá‡¶ü ---
prompt_template = """<|system|>
You are a helpful assistant. Answer the user's question based only on the following context:
{context}
<|end|>
<|user|>
{question}
<|end|>
<|assistant|>"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

# --- Streamlit ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ ---

# ‡ßß. ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
embeddings = load_embedding_model()
llm = load_llm()

# ‡ß®. ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßÅ‡¶®
if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# ‡ß©. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡¶æ‡¶∞
with st.sidebar:
    st.header("‡ßß. PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®")
    uploaded_file = st.file_uploader("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ PDF ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®", type="pdf")
    
    if uploaded_file:
        if st.button("‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®"):
            with st.spinner("PDF ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá... (OCR ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶∏‡¶π) ‡¶è‡¶ü‡¶æ‡¶§‡ßá ‡¶∏‡¶Æ‡ßü ‡¶≤‡¶æ‡¶ó‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§"):
                # RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
                vector_store = create_rag_pipeline(uploaded_file, embeddings)
                
                if vector_store and llm:
                    # RAG ‡¶ö‡ßá‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßá‡¶∂‡¶®‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
                    st.session_state.qa_chain = RetrievalQA.from_chain_type(
                        llm=llm,
                        chain_type="stuff",
                        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
                        chain_type_kwargs={"prompt": PROMPT},
                        return_source_documents=True
                    )
                    st.session_state.messages = [] # ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶π‡¶≤‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶∞‡¶ø‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
                    st.success(f"'{uploaded_file.name}' ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶™‡¶®‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§")
                else:
                    st.error("RAG ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨ ‡¶π‡ßü‡¶®‡¶ø‡•§")

# ‡ß™. ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏
st.header("‡ß®. ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®")

if not st.session_state.qa_chain:
    st.warning("‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶∏‡¶æ‡¶á‡¶°‡¶¨‡¶æ‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç '‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®' ‡¶¨‡¶æ‡¶ü‡¶®‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

# ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ‡¶á‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶®‡¶ø‡¶®
if prompt := st.chat_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®...", disabled=(not st.session_state.qa_chain)):
    # ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI-‡¶è‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
    with st.chat_message("assistant"):
        with st.spinner("‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá..."):
            try:
                result = st.session_state.qa_chain.invoke(prompt)
                response = result['result']
                
                # ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã (‡¶ê‡¶ö‡ßç‡¶õ‡¶ø‡¶ï)
                if result.get('source_documents'):
                    with st.expander("‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®"):
                        for i, doc in enumerate(result['source_documents']):
                            page = doc.metadata.get('page_number', 'N/A')
                            st.write(f"**‡¶∏‡ßã‡¶∞‡ßç‡¶∏ {i+1} (Page: {page})**")
                            st.caption(f"{doc.page_content[:200]}...")
                
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                st.error(f"An error occurred while generating response: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})
